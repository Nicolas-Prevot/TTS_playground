{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, librosa\n",
    "import soundfile as sf\n",
    "from scipy.io.wavfile import write as write_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Utilisateur/.cache\\torch\\hub\\Camb-ai_mars5-tts_master\n",
      "c:\\Users\\Utilisateur\\miniconda3\\envs\\tts_tuto\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "c:\\Users\\Utilisateur\\miniconda3\\envs\\tts_tuto\\Lib\\site-packages\\vocos\\pretrained.py:70: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "mars5, config_class = torch.hub.load('Camb-ai/mars5-tts', 'mars5_english', trust_repo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars5.sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, sr = librosa.load('../data/test2.wav',sr=mars5.sr, mono=True)\n",
    "wav = torch.from_numpy(wav)\n",
    "ref_transcript = \"Exploring the ancient ruins, they stumbled upon a hidden chamber, revealing artifacts that shed new light on a forgotten civilization.\"  # \"Bonjour tout le monde, je m'appelle Nicolas et je suis ici pour vous parler de mon nouveau papier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Utilisateur\\miniconda3\\envs\\tts_tuto\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 2625, 8]) | new x_known: torch.Size([1, 2625, 8]) . Base prompt: torch.Size([1, 900, 8]). New padding mask: torch.Size([1, 2625]) | m shape: torch.Size([1, 2625, 8])\n"
     ]
    }
   ],
   "source": [
    "deep_clone = True\n",
    "# Below you can tune other inference settings, like top_k, temperature, top_p, etc...\n",
    "cfg = config_class(deep_clone=deep_clone, rep_penalty_window=100, top_k=100, temperature=0.7, freq_penalty=3)\n",
    "\n",
    "ar_codes, output_audio = mars5.tts(\"The old sage imparted his wisdom to the young seeker, teaching that true knowledge comes not from books but from life’s experiences.\", wav, ref_transcript, cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.write('../data/output_audio2.wav', output_audio.numpy(), mars5.sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = {\n",
    "    # \"Discovery\": \"Exploring the ancient ruins, they stumbled upon a hidden chamber, revealing artifacts that shed new light on a forgotten civilization.\",\n",
    "    \"Adventure\": \"Embarking on a journey through dense forests and treacherous mountains, they discovered the true meaning of friendship and perseverance.\",\n",
    "    \"Innovation\": \"The scientist’s groundbreaking invention promised to revolutionize renewable energy, offering a sustainable future for generations to come.\",\n",
    "    \"Mystery\": \"In the heart of the city, a series of strange events unfolded, leaving detectives puzzled and residents in fear of the unknown.\",\n",
    "    \"Courage\": \"Facing insurmountable odds, the small village banded together, showing incredible bravery in the face of a powerful and relentless enemy.\",\n",
    "    \"Betrayal\": \"Trust shattered, he realized his closest ally had been deceiving him all along, leading to a confrontation that would change everything.\",\n",
    "    \"Hope\": \"Amidst the chaos and destruction, a single act of kindness sparked a glimmer of hope, inspiring others to believe in a better tomorrow.\",\n",
    "    \"Transformation\": \"The caterpillar’s journey to become a butterfly mirrored her own transformation, finding strength and beauty in unexpected places.\",\n",
    "    \"Wisdom\": \"The old sage imparted his wisdom to the young seeker, teaching that true knowledge comes not from books but from life’s experiences.\",\n",
    "    \"Victory\": \"Against all expectations, the underdog team triumphed in the championship, their hard work and determination finally paying off.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1749, 8]) | new x_known: torch.Size([1, 1749, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1749]) | m shape: torch.Size([1, 1749, 8])\n",
      "Saved audio for 'Discovery' as ../data/Discovery.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1478, 8]) | new x_known: torch.Size([1, 1478, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1478]) | m shape: torch.Size([1, 1478, 8])\n",
      "Saved audio for 'Adventure' as ../data/Adventure.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1667, 8]) | new x_known: torch.Size([1, 1667, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1667]) | m shape: torch.Size([1, 1667, 8])\n",
      "Saved audio for 'Innovation' as ../data/Innovation.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1593, 8]) | new x_known: torch.Size([1, 1593, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1593]) | m shape: torch.Size([1, 1593, 8])\n",
      "Saved audio for 'Mystery' as ../data/Mystery.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1858, 8]) | new x_known: torch.Size([1, 1858, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1858]) | m shape: torch.Size([1, 1858, 8])\n",
      "Saved audio for 'Courage' as ../data/Courage.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1496, 8]) | new x_known: torch.Size([1, 1496, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1496]) | m shape: torch.Size([1, 1496, 8])\n",
      "Saved audio for 'Betrayal' as ../data/Betrayal.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1562, 8]) | new x_known: torch.Size([1, 1562, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1562]) | m shape: torch.Size([1, 1562, 8])\n",
      "Saved audio for 'Hope' as ../data/Hope.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1554, 8]) | new x_known: torch.Size([1, 1554, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1554]) | m shape: torch.Size([1, 1554, 8])\n",
      "Saved audio for 'Transformation' as ../data/Transformation.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1517, 8]) | new x_known: torch.Size([1, 1517, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1517]) | m shape: torch.Size([1, 1517, 8])\n",
      "Saved audio for 'Wisdom' as ../data/Wisdom.wav\n",
      "Note: using deep clone. Assuming input `c_phones` is concatenated prompt and output phones. Also assuming no padded indices in `c_codes`.\n",
      "New x: torch.Size([1, 1687, 8]) | new x_known: torch.Size([1, 1687, 8]) . Base prompt: torch.Size([1, 450, 8]). New padding mask: torch.Size([1, 1687]) | m shape: torch.Size([1, 1687, 8])\n",
      "Saved audio for 'Victory' as ../data/Victory.wav\n"
     ]
    }
   ],
   "source": [
    "for title, text in sentences.items():\n",
    "    ar_codes, output_audio = mars5.tts(text, wav, ref_transcript, cfg=cfg)\n",
    "    filename = f\"../data/{title}.wav\"\n",
    "    sf.write(filename, output_audio.numpy(), 24000)\n",
    "    print(f\"Saved audio for '{title}' as {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_tuto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
